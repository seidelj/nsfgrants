<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>I-Corps: Computer Vision for Tracking People in Different Scenarios</AwardTitle>
    <AwardEffectiveDate>06/01/2015</AwardEffectiveDate>
    <AwardExpirationDate>02/29/2016</AwardExpirationDate>
    <AwardAmount>50000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07070000</Code>
      <Directorate>
        <LongName>Directorate For Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Industrial Innovation &amp; Partnersh</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Rathindra DasGupta</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>A large retail chain with numerous stores in several shopping malls (e.g., Bed, Bath &amp; Beyond) may devise advertising campaigns in which they display new items or feature promotional items within each store, and then endeavor to evaluate the success of each campaign. In addition to tracking their revenues, the retailer may seek to measure traffic outside each store, the percentage of people who enter the store, how people move throughout the store, time spent in each location throughout the store, etc. While there are tools and technologies currently available to assist with these efforts, they provide very crude data, resulting in an unmet market need. Today, infrared cameras are used to address the needs described above, but are crude in their estimates. Other existing solutions include mounted cameras in specific locations to spot people in a given part of the store (e.g., hot zones or entrances to count people entering). They are also used to distinguish different types of people (kids versus adults). But these existing methods and technologies are unreliable and unadaptable (e.g., adjusting to occlusions or changes in natural light over time to get an accurate count), and therefore fail to deliver accurate results to the retail chains.&lt;br/&gt;&lt;br/&gt;This I-Corps team is developing technology that addresses this market need in new and innovative ways, using technology not present in products today and providing more accurate and higher fidelity data than existing solutions on the market. The solution being developed will be easy to use, low cost, and reliable. The proposed technology is applicable to both indoor and outdoor data (accounting for light variations). To develop the solution and business model, the team plans to establish relationships with large retail stores to further assess their needs and where the technology will be best applied. Beyond the retail market, it is believed that the proposed technology will have commercial impact in airports, train/bus stations, and all surveillance activities. Clearly, security is an area where this technology of accurate measurement of people's movements is necessary. With the technology this I-Corps team is developing, retailers will receive more accurate and reliable data about the activities happening in and around their stores (such as the result of a window marketing campaign). This will enable them to improve store operations and provide better experiences to customers.</AbstractNarration>
    <MinAmdLetterDate>05/21/2015</MinAmdLetterDate>
    <MaxAmdLetterDate>05/21/2015</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1542860</AwardID>
    <Investigator>
      <FirstName>Davi</FirstName>
      <LastName>Geiger</LastName>
      <EmailAddress>geiger@cs.nyu.edu</EmailAddress>
      <StartDate>05/21/2015</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>New York University</Name>
      <CityName>NEW YORK</CityName>
      <ZipCode>100121019</ZipCode>
      <PhoneNumber>2129982121</PhoneNumber>
      <StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>New York</StateName>
      <StateCode>NY</StateCode>
    </Institution>
  </Award>
</rootTag>
