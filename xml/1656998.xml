<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>CRII: RI: Methods for Learning and Recovering Partially Embedded Logical Representations for Question Answering</AwardTitle>
<AwardEffectiveDate>08/15/2017</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardAmount>175000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05020000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Div Of Information &amp; Intelligent Systems</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Donald T. Langendoen</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Providing effective access to non-experts to the ever-increasing amount of publicly available information is a challenging open problem. While search has been the dominant mode of access, it does not support complex queries, and provides a poor interface for natural language interaction with the growing number of virtual and embodied agents. The goal of this project is to develop representations and learning methods to enable systems that can respond accurately to complex natural language questions. This work will advance the field of natural language processing and have impact through the development of algorithms, linguistic representations, and applications, including natural language question answering interfaces for large knowledge bases. &lt;br/&gt;&lt;br/&gt;A common approach to question answering, commonly known as semantic parsing, is to map questions to expressive logical representations. This is a challenging task that requires deriving the meaning of words, and combining them following the compositional structure of the sentence. Despite increasing research attention, building systems capable of such understanding requires significant expertise, and state-of-the-art systems are limited by the coverage of existing manually-curated knowledge bases, largely failing to benefit from unstructured text. This project proposes an approach that will (a) significantly reduce the engineering effort and expertise required to build question answering systems, (b) generalize beyond curated data to learn to answer complex factoid questions without a knowledge base, and (c) lay the foundations for a new general approach to recover semantic meaning. The key to this approach is the integration of logical and embedded representations of meaning. It is expected that mapping sentences to representations that combine logical and embedded elements will greatly reduce the amount of representation engineering required, enable complex sentence-level reasoning, and allow effective learning from raw text without access to a structured knowledge base.</AbstractNarration>
<MinAmdLetterDate>03/01/2017</MinAmdLetterDate>
<MaxAmdLetterDate>03/01/2017</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1656998</AwardID>
<Investigator>
<FirstName>Yoav</FirstName>
<LastName>Artzi</LastName>
<EmailAddress>yoav@cs.cornell.edu</EmailAddress>
<StartDate>03/01/2017</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Cornell University</Name>
<CityName>Ithaca</CityName>
<ZipCode>148502820</ZipCode>
<PhoneNumber>6072555014</PhoneNumber>
<StreetAddress>373 Pine Tree Road</StreetAddress>
<CountryName>United States</CountryName>
<StateName>New York</StateName>
<StateCode>NY</StateCode>
</Institution>
<ProgramElement>
<Code>026Y</Code>
<Text>CRII CISE Research Initiation</Text>
</ProgramElement>
<ProgramReference>
<Code>7495</Code>
<Text>ROBUST INTELLIGENCE</Text>
</ProgramReference>
<ProgramReference>
<Code>8228</Code>
<Text>CISE Resrch Initiatn Initiatve</Text>
</ProgramReference>
</Award>
</rootTag>
