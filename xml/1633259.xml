<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>BIGDATA: Collaborative Research: IA: BirdVox: Automating Acoustic Monitoring of Migrating Bird Species</AwardTitle>
    <AwardEffectiveDate>10/01/2016</AwardEffectiveDate>
    <AwardExpirationDate>09/30/2019</AwardExpirationDate>
    <AwardAmount>612383</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Sylvia J. Spengler</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Current bioacoustic monitoring of natural environments requires processing by humans to extract information content from recordings. Thus human processing creates a fundamental bottleneck in which data collection far outpaces capabilities to extract relevant and desired information. Bioacoustic research on automatic species classification in natural environments can be broadly divided into two groups: distinguishing a predefined set of known species from audio clips and extracting species as events that occur in a continuous audio stream. Both classification techniques have their specific problems--many of the data used distinguishing predefined species are recorded under "studio" conditions and not extensible to natural conditions, while processing of continuous audio streams generate many false positives. &lt;br/&gt;&lt;br/&gt;To overcome these challenges we will take a multi-tiered approach: Analyzing a data set consisting of full-night recordings from 10 recording units over 100 nights. Building a web-enabled software to engage citizen scientists to identify the flight calls, providing us with a large and extensive model training dataset. Developing novel convolutional deep-learning networks, which are well suited for analysis of complex auditory scenes. Visualizing patterns detected and classified flight calls in space and time to produce novel information about the bird migration. Comparing model-generated acoustic data with radar, video, and direct visual citizen science datasets to produce the most comprehensive accounts of nocturnal bird migration possible. The combination of domain knowledge in bird vocalizations, engaging citizen scientists to allow development of large well annotated training datasets, and taking a novel deep-learning approach, will finally resolve the machine classification of acoustic signals in natural environments.</AbstractNarration>
    <MinAmdLetterDate>09/13/2016</MinAmdLetterDate>
    <MaxAmdLetterDate>09/13/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1633259</AwardID>
    <Investigator>
      <FirstName>Juan</FirstName>
      <LastName>Bello</LastName>
      <EmailAddress>jpbello@nyu.edu</EmailAddress>
      <StartDate>09/13/2016</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>New York University</Name>
      <CityName>NEW YORK</CityName>
      <ZipCode>100121019</ZipCode>
      <PhoneNumber>2129982121</PhoneNumber>
      <StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>New York</StateName>
      <StateCode>NY</StateCode>
    </Institution>
    <ProgramElement>
      <Code>8083</Code>
      <Text>Big Data Science &amp;Engineering</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7433</Code>
      <Text>CyberInfra Frmwrk 21st (CIF21)</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>8083</Code>
      <Text>Big Data Science &amp;Engineering</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9102</Code>
      <Text>WOMEN, MINORITY, DISABLED, NEC</Text>
    </ProgramReference>
  </Award>
</rootTag>
