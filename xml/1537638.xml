<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Queues in Cloud Computing</AwardTitle>
    <AwardEffectiveDate>09/01/2015</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2018</AwardExpirationDate>
    <AwardAmount>300000</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>07030000</Code>
      <Directorate>
        <LongName>Directorate For Engineering</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Civil, Mechanical, &amp; Manufact Inn</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Donald Hearn</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Cloud computing is a modern paradigm where computing tasks are performed on a subset of servers which coexist in a large distributed network of computers, called clouds. The number of computers in these clouds is rapidly increasing, surpassing hundreds of thousands today. Making large and parallel computing facilities generically available is desirable both from the business as well as the scientific perspective. Specifically, researchers today do not need to own an expensive supercomputer for studying complex systems, since they can tap into the cloud for hours, weeks or months at a time, well below the cost that they would need to pay to maintain a much smaller facility. This research will provide new tractable mathematical techniques for the analysis and efficient control of these large-scale systems.&lt;br/&gt;&lt;br/&gt;The complexity of many of the systems used today for the distributed processing of large computing jobs makes it difficult to understand the impact of server information on designing scheduling protocols. The research done through this award will start by analyzing a mathematically tractable model for a network of servers where jobs are, upon arrival, split into a number of pieces, which are then assigned/queued at randomly chosen servers. The main characteristic of the model is that all pieces of a job must receive service in a synchronized fashion. This model, combined with a second model where jobs wait for the required number of servers to become available, will provide a benchmark for designing as well as quantifying the gains of practical scheduling policies. By noting that the two models constitute the extreme cases of having no server information versus having full, centralized information, this work will essentially provide a price for this knowledge, which will play an important role in designing future cloud systems.</AbstractNarration>
    <MinAmdLetterDate>08/22/2015</MinAmdLetterDate>
    <MaxAmdLetterDate>08/22/2015</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1537638</AwardID>
    <Investigator>
      <FirstName>Predrag</FirstName>
      <LastName>Jelenkovic</LastName>
      <EmailAddress>predrag@ee.columbia.edu</EmailAddress>
      <StartDate>08/22/2015</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Mariana</FirstName>
      <LastName>Olvera-Cravioto</LastName>
      <EmailAddress>molvera@ieor.columbia.edu</EmailAddress>
      <StartDate>08/22/2015</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Columbia University</Name>
      <CityName>NEW YORK</CityName>
      <ZipCode>100276902</ZipCode>
      <PhoneNumber>2128546851</PhoneNumber>
      <StreetAddress>2960 Broadway</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>New York</StateName>
      <StateCode>NY</StateCode>
    </Institution>
  </Award>
</rootTag>
