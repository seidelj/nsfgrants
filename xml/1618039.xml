<?xml version="1.0" encoding="UTF-8"?>
<rootTag>
<Award>
<AwardTitle>SHF: Small: Latency Tolerance Aware Runtime Optimization for General-Purpose GPU Architectures</AwardTitle>
<AwardEffectiveDate>08/01/2016</AwardEffectiveDate>
<AwardExpirationDate>07/31/2019</AwardExpirationDate>
<AwardAmount>346000</AwardAmount>
<AwardInstrument>
<Value>Standard Grant</Value>
</AwardInstrument>
<Organization>
<Code>05010000</Code>
<Directorate>
<LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
</Directorate>
<Division>
<LongName>Division of Computing and Communication Foundations</LongName>
</Division>
</Organization>
<ProgramOfficer>
<SignBlockName>Tao Li</SignBlockName>
</ProgramOfficer>
<AbstractNarration>Computing touches almost all aspects of our modern day lives -- from gene sequencing to physics simulations to powering the Internet, from real-time image and voice recognition to predicting stock market trends. The programmability advancement of high-performance accelerators, such as graphics processors, has enabled a large, diverse set of general-purpose algorithms to enjoy performance acceleration on GPUs. However, because the unique latency tolerance GPU design feature is often not taken into account, the state-of-the-art solutions lead to sub-optimal performance improvement. The proposed Latency Tolerance Aware runtime optimization framework (LATTE) can help the computing industry realize its high performance and high energy efficiency vision. Performance acceleration of important general-purpose algorithms with large and diverse input data sets has a profound impact on the advancement of all research domains and on society. The research agenda is complemented by an education agenda focusing on heterogeneous computing.&lt;br/&gt;&lt;br/&gt;The LATTE framework proposed in this project aims to explore and propose architectural solutions and to create system supports to accelerate the execution of important general-purpose applications on GPUs. The proposed LATTE runtime optimization framework revolves around identifying and designing optimization techniques that are fully latency tolerance aware in order to maximize performance improvement for GPGPUs. The pitfalls for directly adapting CPU-centric optimization to GPU architectures are analyzed and used to motivate the need to proactively manage the highly time-multiplexed computation and memory resources considering the critical latency tolerance characteristic of GPUs. The findings can serve as foundations for future performance optimization research to avoid blind replication, leading to important scientific research contributions for GPGPU acceleration.</AbstractNarration>
<MinAmdLetterDate>07/28/2016</MinAmdLetterDate>
<MaxAmdLetterDate>05/15/2017</MaxAmdLetterDate>
<ARRAAmount/>
<AwardID>1618039</AwardID>
<Investigator>
<FirstName>Carole-Jean</FirstName>
<LastName>Wu</LastName>
<EmailAddress>carole-jean.wu@asu.edu</EmailAddress>
<StartDate>07/28/2016</StartDate>
<EndDate/>
<RoleCode>Principal Investigator</RoleCode>
</Investigator>
<Institution>
<Name>Arizona State University</Name>
<CityName>TEMPE</CityName>
<ZipCode>852816011</ZipCode>
<PhoneNumber>4809655479</PhoneNumber>
<StreetAddress>ORSPA</StreetAddress>
<CountryName>United States</CountryName>
<StateName>Arizona</StateName>
<StateCode>AZ</StateCode>
</Institution>
<ProgramElement>
<Code>7798</Code>
<Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
</ProgramElement>
<ProgramReference>
<Code>7923</Code>
<Text>SMALL PROJECT</Text>
</ProgramReference>
<ProgramReference>
<Code>7941</Code>
<Text>COMPUTER ARCHITECTURE</Text>
</ProgramReference>
<ProgramReference>
<Code>9251</Code>
<Text>RES EXPER FOR UNDERGRAD-SUPPLT</Text>
</ProgramReference>
</Award>
</rootTag>
