<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>CIF: Small: Bypassing the L1 Norm: Non-Convex Regularization, Convex Optimization, and Sparse Signal Processing</AwardTitle>
    <AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2018</AwardExpirationDate>
    <AwardAmount>315175</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05010000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Computing and Communication Foundations</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>John Cozzens</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Numerous problems in nonlinear signal processing and data science are successfully tackled through their formulation as ill-conditioned inverse problems. Such problems arise in medical imaging, speech and audio processing, biomedical time-series analysis, manufacturing, and remote sensing. Many ongoing advances in these fields are based on sparse regularization (i.e., the modeling of data as highly compressible when appropriately transformed). This research program aims to advance mathematical and computational tools to pose and solve ill-conditioned inverse problems by developing new techniques for sparse regularization. &lt;br/&gt;&lt;br/&gt;Convex formulations of problems arising in science and engineering are attractive because one may leverage a wealth of algorithms that are globally convergent and computationally efficient, even for large-scale non-smooth problems. While the L1 norm is a cornerstone of convex sparse regularization, it tends to under-estimate signal values. Non-convex sparse regularization is therefore a popular and valuable alternative; however, it is hampered by several complications: the optimal solution is generally a discontinuous function of the data and the objective function to be minimized generally possesses many sub-optimal local minima which can entrap optimization algorithms. &lt;br/&gt;&lt;br/&gt;This research aims to exploit the effectiveness of non-convex sparse regularization without forgoing the principles of convex optimization, by applying the principle of convex relaxation to the objective function as a whole, rather than to the regularizer alone. In particular, this program undertakes the development of new non-separable non-convex penalty functions that ensure the convexity of the objective function (comprising data fidelity and sparse regularization terms) to be minimized.</AbstractNarration>
    <MinAmdLetterDate>07/08/2015</MinAmdLetterDate>
    <MaxAmdLetterDate>07/08/2015</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1525398</AwardID>
    <Investigator>
      <FirstName>Ivan</FirstName>
      <LastName>Selesnick</LastName>
      <EmailAddress>selesi@nyu.edu</EmailAddress>
      <StartDate>07/08/2015</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>New York University</Name>
      <CityName>NEW YORK</CityName>
      <ZipCode>100121019</ZipCode>
      <PhoneNumber>2129982121</PhoneNumber>
      <StreetAddress>70 WASHINGTON SQUARE S</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>New York</StateName>
      <StateCode>NY</StateCode>
    </Institution>
  </Award>
</rootTag>
