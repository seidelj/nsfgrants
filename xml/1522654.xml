<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Acceleration Techniques for Lower-Order Algorithms in Nonlinear Optimization</AwardTitle>
    <AwardEffectiveDate>08/01/2015</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2018</AwardExpirationDate>
    <AwardAmount>177771</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>03040000</Code>
      <Directorate>
        <LongName>Direct For Mathematical &amp; Physical Scien</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Mathematical Sciences</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Junping Wang</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>This project focuses on developing efficient innovative acceleration techniques and their underlying theories for the algorithms in nonlinear optimization. The acceleration techniques and algorithms developed in this project will have broad impact in many areas of computational science, including imaging/signal processing, optimal control, computer vision, petroleum engineering, topology optimization, and electronic structure computations. The algorithms developed in this research will be made publicly available on the web and will be applied in solving various computational problems. In addition, the student involved in this project will have excellent opportunities to participate in interdisciplinary research.&lt;br/&gt;&lt;br/&gt;The research will include developing subspace techniques for nonlinear conjugate gradient method and accelerated nonlinear conjugate gradient methods with theoretically guaranteed optimal global complexity. A framework of inexact alternating direction method of multipliers (ADMM) will also be developed, in which multiple steps are allowed to solve the subproblem to an adaptive accuracy, while still maintaining global convergence even when the problem has more than two blocks. The project will also study acceleration strategies for gradient based stochastic optimization. In particular, adaptive strategies for choosing sample points and extracting quasi-Newton information based on the obtained stochastic information will be explored. In addition, a novel dual active set approach will be developed for solving smooth large-scale nonlinear optimization. For example, in projection on polyhedra, an algorithm can be developed to approximately identify the active linear constraints, while an asymptotically faster algorithm can be used to compute a high accuracy solution.</AbstractNarration>
    <MinAmdLetterDate>07/28/2015</MinAmdLetterDate>
    <MaxAmdLetterDate>07/28/2015</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1522654</AwardID>
    <Investigator>
      <FirstName>Hongchao</FirstName>
      <LastName>Zhang</LastName>
      <EmailAddress>hozhang@math.lsu.edu</EmailAddress>
      <StartDate>07/28/2015</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>Louisiana State University &amp; Agricultural and Mechanical College</Name>
      <CityName>Baton Rouge</CityName>
      <ZipCode>708032701</ZipCode>
      <PhoneNumber>2255782760</PhoneNumber>
      <StreetAddress>202 Himes Hall</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Louisiana</StateName>
      <StateCode>LA</StateCode>
    </Institution>
  </Award>
</rootTag>
