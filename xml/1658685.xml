<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>Doctoral Dissertation Research in DRMS: On the Evaluation of Beliefs: A Method for Assessing Credibility in Subjective Probability Judgment</AwardTitle>
    <AwardEffectiveDate>02/01/2017</AwardEffectiveDate>
    <AwardExpirationDate>01/31/2018</AwardExpirationDate>
    <AwardAmount>15915</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04050000</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Divn Of Social and Economic Sciences</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Robert E. O'Connor</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>Uncertainty is a pervasive feature of the world we live in. In the face of uncertainty, decision makers cannot observe probabilities directly and must instead base their choices on subjective beliefs. These beliefs are often expressed as subjective probability judgments (SPJs), and their quality is constrained by the knowledge of the individuals who hold them. Consequently, decision making under uncertainty requires that decision makers account for the quality of the judgments at their disposal. In many of the areas that concern us the most (e.g., health, safety, and the protection of the environment), however, decisions often hinge on the likelihood of single events such as the success of a surgery, the growth of a mutual fund, or the melting of the polar ice caps. As a result, evaluating SPJs is either impractical or impossible in many areas of decision making. To circumvent this issue, research on judgment under uncertainty has often been forced to evaluate SPJs relative to (a) laboratory situations in which base rates or relative frequencies are known; (b) sets of judgments for which probability theory demands some qualitative ordering or relationship; or (c) the observed outcomes of uncertain events. Thus, while informative, the extensive body of research on judgment under uncertainty is often of little use to real-world decision makers when attempting to identify their best course of action, ex ante. This dissertation develops a method for assessing the quality of SPJs when benchmarks (e.g., base rates, outcomes) are unknown. &lt;br/&gt;&lt;br/&gt;Specifically, the present research develops a method for measuring the degree to which an individual's SPJs tend to agree with the optimized, aggregate "wisdom of the crowds", also known as credibility. To do this, we will conduct several probability forecasting tournaments and regress each individual's SPJs on optimized aggregates, calculated using methods developed by the Good Judgment Project. The estimated parameters of these models are indices of an individual's bias; expertise; and acuity (measured as the standard error of the regression) in subjective probability judgment. This work improves upon previous methods for assessing uncertainty in at least two ways. First, the credibility framework developed here can be used to evaluate SPJs when normative benchmarks (e.g., base rates, outcomes, repeated measurements) are unavailable or unknown. Second, preliminary evidence suggests that modeling judgments in this way can provide decision makers with an empirical method for correcting "errors" and "biases" in subjective probability judgment. Additionally, because predictions in domains such as military intelligence, climate science, and epidemiology tend be highly uncertain and errors prohibitively costly, decision makers in these domains may benefit from even marginal gains in their ability to evaluate SPJs. The present research facilitates this evaluation by providing straightforward measures of a source's quality, independent of empirical outcomes. In doing so, this research improves the quality and evaluability of decision making across a wide variety of domains.</AbstractNarration>
    <MinAmdLetterDate>02/01/2017</MinAmdLetterDate>
    <MaxAmdLetterDate>02/01/2017</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1658685</AwardID>
    <Investigator>
      <FirstName>Joshua</FirstName>
      <LastName>Baker</LastName>
      <EmailAddress>ibak@sas.upenn.edu</EmailAddress>
      <StartDate>02/01/2017</StartDate>
      <EndDate/>
      <RoleCode>Co-Principal Investigator</RoleCode>
    </Investigator>
    <Investigator>
      <FirstName>Jonathan</FirstName>
      <LastName>Baron</LastName>
      <EmailAddress>baron@psych.upenn.edu</EmailAddress>
      <StartDate>02/01/2017</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Pennsylvania</Name>
      <CityName>Philadelphia</CityName>
      <ZipCode>191046205</ZipCode>
      <PhoneNumber>2158987293</PhoneNumber>
      <StreetAddress>Research Services</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Pennsylvania</StateName>
      <StateCode>PA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>1321</Code>
      <Text>DECISION RISK &amp; MANAGEMENT SCI</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>9179</Code>
      <Text>GRADUATE INVOLVEMENT</Text>
    </ProgramReference>
  </Award>
</rootTag>
