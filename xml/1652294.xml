<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>CAREER: In-Situ Compute Memories for Accelerating Data Parallel Applications</AwardTitle>
    <AwardEffectiveDate>02/01/2017</AwardEffectiveDate>
    <AwardExpirationDate>01/31/2022</AwardExpirationDate>
    <AwardAmount>154326</AwardAmount>
    <AwardInstrument>
      <Value>Continuing grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05010000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Division of Computing and Communication Foundations</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Almadena Y. Chtchelkanova</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>As computing today is dominated by Big Data, there is a strong impetus for specialization for this important domain. Performance of these data-centric applications depends critically on efficient access and processing of data. These applications tend to be highly data-parallel and deal with large amounts. Recent studies show that by the year 2020, data production from individuals and corporations is expected to grow to 73.5 zetabytes, a 4.4× increase from the year 2015. In addition, they tend to expend disproportionately large fraction of time and energy in moving data from storage to compute units, and in instruction processing, when compared to the actual computation. This research seeks to design specialized data-centric computing systems that dramatically reduce these overheads. &lt;br/&gt;&lt;br/&gt;In a general-purpose computing system, the majority of the aggregate die area (over 90%) is devoted for storing and retrieving information at several levels in the memory hierarchy: on-chip caches, main memory (DRAM), and non-volatile memory (NVM). The central vision of this research is to create in-situ compute memories, which re-purpose the elements used in these storage structures and transform them into active computational units. In contrast to prior processing in memory approaches, which augment logic outside the memory arrays, the underpinning principle behind in-situ compute memories is to enable computation in-place within each memory array, without transferring the data in or out of it. Such a transformation could unlock massive data-parallel compute capabilities (up to 100×), and reduce energy spent in data movement through various levels of memory hierarchy (up to 20×), thereby directly address the needs of data-centric applications. This work develops in-situ compute memory technology, adapts the system software stack and re-designs data-centric applications to take advantage of those memories.</AbstractNarration>
    <MinAmdLetterDate>01/24/2017</MinAmdLetterDate>
    <MaxAmdLetterDate>01/24/2017</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1652294</AwardID>
    <Investigator>
      <FirstName>Reetuparna</FirstName>
      <LastName>Das</LastName>
      <EmailAddress>reetudas@umich.edu</EmailAddress>
      <StartDate>01/24/2017</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Michigan Ann Arbor</Name>
      <CityName>Ann Arbor</CityName>
      <ZipCode>481091274</ZipCode>
      <PhoneNumber>7347636438</PhoneNumber>
      <StreetAddress>3003 South State St. Room 1062</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Michigan</StateName>
      <StateCode>MI</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7798</Code>
      <Text>SOFTWARE &amp; HARDWARE FOUNDATION</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>1045</Code>
      <Text>CAREER: FACULTY EARLY CAR DEV</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7942</Code>
      <Text>HIGH-PERFORMANCE COMPUTING</Text>
    </ProgramReference>
  </Award>
</rootTag>
