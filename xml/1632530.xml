<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>The Multisensory Training Benefit for Speech and Speaker Perception</AwardTitle>
    <AwardEffectiveDate>08/15/2016</AwardEffectiveDate>
    <AwardExpirationDate>07/31/2019</AwardExpirationDate>
    <AwardAmount>349458</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>04040000</Code>
      <Directorate>
        <LongName>Direct For Social, Behav &amp; Economic Scie</LongName>
      </Directorate>
      <Division>
        <LongName>Division Of Behavioral and Cognitive Sci</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Betty H. Tuller</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>When listening to someone in a noisy environment, you can better understand what they say when you look at their face while they talk. Providing audiovisual information about speech, not just audio information alone, has also been shown to help infants develop language. In some cases, using audiovisual information to train speech perception helps adults learn a second language, helps aphasic patients produce more fluent speech, and improves perception of heard speech by listeners with hearing impairments. This last group is of particular interest because roughly a third of U.S. adults have some degree of hearing impairment.&lt;br/&gt;&lt;br/&gt;While this audiovisual training advantage is well established, it is unclear how exactly the effect works. Research will be conducted to determine the principles behind the effect, how and why it works, and in what circumstances. The investigators also explore why audiovisual training typically does not help hearing-impaired individuals who received cochlear implants after early childhood. One possibility is that their years of experience before receiving the cochlear implant required a heavy reliance on lip reading, leaving them over-reliant on information from the talker during audiovisual training. &lt;br/&gt;&lt;br/&gt;The specific issues examined are rooted in theories of multisensory learning. These issues include: a) whether simultaneous auditory and visual speech is necessary for the training advantage to occur; b) to what degree the advantage generalizes across different talkers for a given listener; and c) whether the use of other sensory channels, in this case audio and touch, results in a similar training advantage. Parallel experiments will be carried out with both normal-hearing individuals and individuals who received cochlear implants after early childhood. The research should not only improve speech perception training regimes for individuals in this clinical population but should also improve our understanding of typical speech and talker perception. More generally, the research should inform our understanding of perceptual learning and plasticity, as well as multisensory integration.</AbstractNarration>
    <MinAmdLetterDate>08/18/2016</MinAmdLetterDate>
    <MaxAmdLetterDate>08/18/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1632530</AwardID>
    <Investigator>
      <FirstName>Lawrence</FirstName>
      <LastName>Rosenblum</LastName>
      <EmailAddress>lawrence.rosenblum@ucr.edu</EmailAddress>
      <StartDate>08/18/2016</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of California-Riverside</Name>
      <CityName>RIVERSIDE</CityName>
      <ZipCode>925211000</ZipCode>
      <PhoneNumber>9518275535</PhoneNumber>
      <StreetAddress>Office of Research</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>California</StateName>
      <StateCode>CA</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7252</Code>
      <Text>PERCEPTION, ACTION &amp; COGNITION</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>7252</Code>
      <Text>Perception, Action and Cognition</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>9179</Code>
      <Text>GRADUATE INVOLVEMENT</Text>
    </ProgramReference>
  </Award>
</rootTag>
