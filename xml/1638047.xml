<?xml version="1.0" encoding="UTF-8"?>

<rootTag>
  <Award>
    <AwardTitle>NRI: Collaborative Research: Sketching Geometry and Physics Informed Inference for Mobile Robot Manipulation in Cluttered Scenes</AwardTitle>
    <AwardEffectiveDate>09/01/2016</AwardEffectiveDate>
    <AwardExpirationDate>08/31/2019</AwardExpirationDate>
    <AwardAmount>400857</AwardAmount>
    <AwardInstrument>
      <Value>Standard Grant</Value>
    </AwardInstrument>
    <Organization>
      <Code>05020000</Code>
      <Directorate>
        <LongName>Direct For Computer &amp; Info Scie &amp; Enginr</LongName>
      </Directorate>
      <Division>
        <LongName>Div Of Information &amp; Intelligent Systems</LongName>
      </Division>
    </Organization>
    <ProgramOfficer>
      <SignBlockName>Reid Simmons</SignBlockName>
    </ProgramOfficer>
    <AbstractNarration>The goal of this project is to improve the ability of robots to manipulate and interact with objects, such as when assisting people to support their daily activities. The key idea is that people can provide robots with important information about their environment and the objects within their environment. In particular, people can use their cognitive skills to name objects, provide an understanding of the geometrical structure of objects, and describe an object's behavior in relation to other objects. Specifically, the project will develop a natural user interface that enables people to provide such information by drawing and sketching on top of the robot's view of the world. Physical simulation will then be used to fill in the missing gaps needed for a robot to complete autonomous manipulation tasks. Thus, the project aims to combine object sketching and physical simulation to better support mobile manipulation tasks as well as learn to perform new manipulation tasks when encountered. The project will support a "Put That There" task, where a user can simply give high-level manipulation commands, with the robot filling in the details necessary to complete the task in a cluttered environment.&lt;br/&gt;&lt;br/&gt;This project aims to improve goal-directed dexterous robotic manipulation in cluttered and unstructured environments through sketching and physical simulation. Robots operating in human environments face considerable uncertainty in perception due to physical contact and occlusions between objects. This project will address such perceptual uncertainty by combining methods for probabilistic inference with natural sketch-based interfaces to extract, label, and automatically infer the geometry, pose, and behavior of objects in complicated scenes. From a human usability perspective, the project addresses how to best create a sketching language and interfaces for intuitive human-in-the-loop extraction of object geometries and behavior from robot sensing. The planned exploration into sketching methods will also explore what underlying representations, raw point clouds, RGB images and video, or RGBD images will be most conducive to supporting accurate geometry extraction and grasp location identification. Given sketched objects, the project will develop probabilistic physically plausible methods for scene estimation that will enable perception for manipulation in cluttered environments. These methods build upon advances in physical simulation to constrain scene estimates to only plausible configurations to both improve estimation accuracy and enable computational tractability. The project will also develop a "Put That There" testbed using a tablet-based web application to support exploration of these concepts as well as act as user studies to evaluate geometry extraction accuracy and the robustness of physics-based scene estimation algorithms.</AbstractNarration>
    <MinAmdLetterDate>08/10/2016</MinAmdLetterDate>
    <MaxAmdLetterDate>08/10/2016</MaxAmdLetterDate>
    <ARRAAmount/>
    <AwardID>1638047</AwardID>
    <Investigator>
      <FirstName>Odest</FirstName>
      <LastName>Jenkins</LastName>
      <EmailAddress>ocj@umich.edu</EmailAddress>
      <StartDate>08/10/2016</StartDate>
      <EndDate/>
      <RoleCode>Principal Investigator</RoleCode>
    </Investigator>
    <Institution>
      <Name>University of Michigan Ann Arbor</Name>
      <CityName>Ann Arbor</CityName>
      <ZipCode>481091274</ZipCode>
      <PhoneNumber>7347636438</PhoneNumber>
      <StreetAddress>3003 South State St. Room 1062</StreetAddress>
      <CountryName>United States</CountryName>
      <StateName>Michigan</StateName>
      <StateCode>MI</StateCode>
    </Institution>
    <ProgramElement>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramElement>
    <ProgramElement>
      <Code>8013</Code>
      <Text>National Robotics Initiative</Text>
    </ProgramElement>
    <ProgramReference>
      <Code>8086</Code>
      <Text>Natl Robotics Initiative (NRI)</Text>
    </ProgramReference>
    <ProgramReference>
      <Code>7495</Code>
      <Text>ROBUST INTELLIGENCE</Text>
    </ProgramReference>
  </Award>
</rootTag>
